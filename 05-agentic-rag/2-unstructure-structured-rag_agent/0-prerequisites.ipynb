{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a61171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "redshift_client = boto3.client('redshift-serverless', region_name=region)\n",
    "redshift_data_client = boto3.client('redshift-data', region_name=region)\n",
    "iam_client = boto3.client('iam')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5d40a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using suffix: 0144320\n"
     ]
    }
   ],
   "source": [
    "# Generate unique suffix for resource names\n",
    "current_time = time.time()\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "suffix = f\"{timestamp_str}\"\n",
    "\n",
    "print(f\"Using suffix: {suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05e550",
   "metadata": {},
   "source": [
    "## Step 1: Download Bedrock Knowledge Base Utilities\n",
    "\n",
    "Lets download the structured knowledge base utility to help with Knowledge Base configuration and creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ab0fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded structured KB utils to utils/structured_knowledge_base.py\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/aws-samples/amazon-bedrock-samples/main/rag/knowledge-bases/features-examples/utils/structured_knowledge_base.py\"\n",
    "target_path = \"utils/structured_knowledge_base.py\"\n",
    "os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "response = requests.get(url)\n",
    "with open(target_path, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "print(f\"Downloaded structured KB utils to {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f62900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.structured_knowledge_base import BedrockStructuredKnowledgeBase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2ccb8",
   "metadata": {},
   "source": [
    "## Step 2: Set up Redshift Serverless Infrastructure\n",
    "\n",
    "Next we will create the necessary Redshift Serverless components: namespace and workgroup. This infrastructure will host our structured data that the Knowledge Base will query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7182df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Namespace: sds-ecommerce-0144320\n",
      "Redshift Workgroup: sds-ecommerce-wg-0144320\n",
      "Database: sds-ecommerce\n",
      "S3 Bucket: sds-ecommerce-redshift-0144320\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Redshift resources\n",
    "REDSHIFT_NAMESPACE = f'sds-ecommerce-{suffix}'\n",
    "REDSHIFT_WORKGROUP = f'sds-ecommerce-wg-{suffix}'\n",
    "REDSHIFT_DATABASE = f'sds-ecommerce'\n",
    "S3_BUCKET = f'sds-ecommerce-redshift-{suffix}'\n",
    "\n",
    "print(f\"Redshift Namespace: {REDSHIFT_NAMESPACE}\")\n",
    "print(f\"Redshift Workgroup: {REDSHIFT_WORKGROUP}\")\n",
    "print(f\"Database: {REDSHIFT_DATABASE}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109b83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created role RedshiftS3AccessRole-0144320\n",
      "Redshift IAM Role ARN: arn:aws:iam::533267284022:role/RedshiftS3AccessRole-0144320\n"
     ]
    }
   ],
   "source": [
    "def create_iam_role_for_redshift():\n",
    "    \"\"\"Create IAM role for Redshift to access S3\"\"\"\n",
    "    try:\n",
    "        # Get account ID\n",
    "        account_id = sts_client.get_caller_identity()['Account']\n",
    "        \n",
    "        # Create IAM role if it doesn't exist\n",
    "        role_name = f'RedshiftS3AccessRole-{suffix}'\n",
    "        try:\n",
    "            role_response = iam_client.get_role(RoleName=role_name)\n",
    "            print(f'Role {role_name} already exists')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "        except iam_client.exceptions.NoSuchEntityException:\n",
    "            trust_policy = {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"redshift.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "            \n",
    "            # Attach necessary policies\n",
    "            iam_client.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\n",
    "            )\n",
    "            \n",
    "            print(f'Created role {role_name}')\n",
    "            return f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'Error creating IAM role: {str(e)}')\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "redshift_role_arn = create_iam_role_for_redshift()\n",
    "print(f\"Redshift IAM Role ARN: {redshift_role_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e59245e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating namespace sds-ecommerce-0144320...\n",
      "Created namespace sds-ecommerce-0144320\n",
      "Waiting for namespace to be available...\n",
      "Namespace sds-ecommerce-0144320 is now available\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_namespace():\n",
    "    \"\"\"Create Redshift Serverless namespace\"\"\"\n",
    "    try:\n",
    "        # Check if namespace already exists\n",
    "        try:\n",
    "            response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "            print(f'Namespace {REDSHIFT_NAMESPACE} already exists')\n",
    "            return response['namespace']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating namespace {REDSHIFT_NAMESPACE}...')\n",
    "        \n",
    "        # Create the namespace\n",
    "        response = redshift_client.create_namespace(\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            adminUsername='admin',\n",
    "            adminUserPassword='TempPassword123!',  # Change this in production\n",
    "            dbName=REDSHIFT_DATABASE,\n",
    "            defaultIamRoleArn=redshift_role_arn,\n",
    "            iamRoles=[redshift_role_arn]\n",
    "        )\n",
    "        \n",
    "        print(f'Created namespace {REDSHIFT_NAMESPACE}')\n",
    "        \n",
    "        # Wait for namespace to be available\n",
    "        print('Waiting for namespace to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                namespace_response = redshift_client.get_namespace(namespaceName=REDSHIFT_NAMESPACE)\n",
    "                status = namespace_response['namespace']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Namespace {REDSHIFT_NAMESPACE} is now available')\n",
    "                    return namespace_response['namespace']\n",
    "                else:\n",
    "                    print(f'Namespace status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking namespace status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for namespace, but proceeding...')\n",
    "        return response['namespace']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating namespace: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create namespace\n",
    "namespace = create_redshift_namespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "372e2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating workgroup sds-ecommerce-wg-0144320...\n",
      "Created workgroup sds-ecommerce-wg-0144320\n",
      "Waiting for workgroup to be available...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup status: CREATING, waiting...\n",
      "Workgroup sds-ecommerce-wg-0144320 is now available\n",
      "Workgroup ARN: arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/7c19387f-5440-4e45-b76a-97e19e6920f5\n"
     ]
    }
   ],
   "source": [
    "def create_redshift_workgroup():\n",
    "    \"\"\"Create Redshift Serverless workgroup\"\"\"\n",
    "    try:\n",
    "        # Check if workgroup already exists\n",
    "        try:\n",
    "            response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "            print(f'Workgroup {REDSHIFT_WORKGROUP} already exists')\n",
    "            return response['workgroup']\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(f'Creating workgroup {REDSHIFT_WORKGROUP}...')\n",
    "        \n",
    "        # Create the workgroup\n",
    "        response = redshift_client.create_workgroup(\n",
    "            workgroupName=REDSHIFT_WORKGROUP,\n",
    "            namespaceName=REDSHIFT_NAMESPACE,\n",
    "            baseCapacity=8,  # Minimum base capacity\n",
    "            enhancedVpcRouting=False,\n",
    "            publiclyAccessible=True,\n",
    "            configParameters=[\n",
    "                {\n",
    "                    'parameterKey': 'enable_user_activity_logging',\n",
    "                    'parameterValue': 'true'\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f'Created workgroup {REDSHIFT_WORKGROUP}')\n",
    "        \n",
    "        # Wait for workgroup to be available\n",
    "        print('Waiting for workgroup to be available...')\n",
    "        max_attempts = 30\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                workgroup_response = redshift_client.get_workgroup(workgroupName=REDSHIFT_WORKGROUP)\n",
    "                status = workgroup_response['workgroup']['status']\n",
    "                if status == 'AVAILABLE':\n",
    "                    print(f'Workgroup {REDSHIFT_WORKGROUP} is now available')\n",
    "                    return workgroup_response['workgroup']\n",
    "                else:\n",
    "                    print(f'Workgroup status: {status}, waiting...')\n",
    "                    time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(f'Error checking workgroup status: {str(e)}, retrying...')\n",
    "                time.sleep(10)\n",
    "        \n",
    "        print('Timeout waiting for workgroup, but proceeding...')\n",
    "        return response['workgroup']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error creating workgroup: {str(e)}')\n",
    "        raise\n",
    "\n",
    "# Create workgroup\n",
    "workgroup = create_redshift_workgroup()\n",
    "workgroup_arn = workgroup['workgroupArn']\n",
    "print(f\"Workgroup ARN: {workgroup_arn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a03caa",
   "metadata": {},
   "source": [
    "## Step 3: Create S3 Bucket and Load Sample Data\n",
    "\n",
    "We will create an S3 bucket to stage our sample e-commerce data before loading it into Redshift tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9976c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket sds-ecommerce-redshift-0144320\n"
     ]
    }
   ],
   "source": [
    "def create_s3_bucket():\n",
    "    \"\"\"Create S3 bucket for data staging\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        print(f'Bucket {S3_BUCKET} already exists')\n",
    "    except:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=S3_BUCKET)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=S3_BUCKET,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f'Created bucket {S3_BUCKET}')\n",
    "        except Exception as e:\n",
    "            print(f'Error creating bucket: {str(e)}')\n",
    "            raise\n",
    "\n",
    "# Create S3 bucket\n",
    "create_s3_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be47dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading sample data files to S3...\n",
      "Uploaded orders.csv (1.8 MB) to S3\n",
      "Uploaded order_items.csv (1.3 MB) to S3\n",
      "Uploaded payments.csv (0.8 MB) to S3\n",
      "Uploaded reviews.csv (0.5 MB) to S3\n",
      "\n",
      "Successfully uploaded all 4 data files to S3\n"
     ]
    }
   ],
   "source": [
    "def upload_sample_data():\n",
    "    \"\"\"Upload sample CSV files to S3\"\"\"\n",
    "    data_files = ['orders.csv', 'order_items.csv', 'payments.csv', 'reviews.csv']\n",
    "    sds_directory = 'sample_data'\n",
    "    \n",
    "    print(\"Uploading sample data files to S3...\")\n",
    "    files_found = 0\n",
    "    \n",
    "    for file_name in data_files:\n",
    "        local_path = os.path.join(sds_directory, file_name)\n",
    "        if os.path.exists(local_path):\n",
    "            # Get file size for informational purposes\n",
    "            file_size = os.path.getsize(local_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            \n",
    "            s3_client.upload_file(local_path, S3_BUCKET, file_name)\n",
    "            print(f'Uploaded {file_name} ({file_size_mb:.1f} MB) to S3')\n",
    "            files_found += 1\n",
    "        else:\n",
    "            print(f'Warning: {local_path} not found')\n",
    "    \n",
    "    if files_found == len(data_files):\n",
    "        print(f\"\\nSuccessfully uploaded all {files_found} data files to S3\")\n",
    "    else:\n",
    "        print(f\"\\nOnly {files_found} out of {len(data_files)} files were found and uploaded\")\n",
    "\n",
    "# Upload sample data\n",
    "upload_sample_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd94c8",
   "metadata": {},
   "source": [
    "## Step 4: Create Redshift Tables and Load Data\n",
    "\n",
    "Now we will create the database tables in Redshift and load our sample e-commerce data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a4b8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_statement(statement_id):\n",
    "    \"\"\"Wait for a Redshift Data API statement to complete\"\"\"\n",
    "    max_attempts = 30\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = redshift_data_client.describe_statement(Id=statement_id)\n",
    "            status = response['Status']\n",
    "            if status == 'FINISHED':\n",
    "                return response\n",
    "            elif status == 'FAILED':\n",
    "                raise Exception(f\"Statement failed: {response.get('Error', 'Unknown error')}\")\n",
    "            elif status == 'CANCELLED':\n",
    "                raise Exception(\"Statement was cancelled\")\n",
    "            else:\n",
    "                print(f\"Statement status: {status}, waiting...\")\n",
    "                time.sleep(5)\n",
    "        except Exception as e:\n",
    "            if 'Statement failed' in str(e) or 'cancelled' in str(e):\n",
    "                raise\n",
    "            print(f\"Error checking statement status: {str(e)}, retrying...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    raise Exception(\"Timeout waiting for statement to complete\")\n",
    "\n",
    "def run_redshift_statement(sql_statement):\n",
    "    \"\"\"Execute a SQL statement in Redshift\"\"\"\n",
    "    try:\n",
    "        response = redshift_data_client.execute_statement(\n",
    "            WorkgroupName=REDSHIFT_WORKGROUP,\n",
    "            Database=REDSHIFT_DATABASE,\n",
    "            Sql=sql_statement\n",
    "        )\n",
    "        statement_id = response['Id']\n",
    "        print(f\"Executing statement: {statement_id}\")\n",
    "        result = wait_for_statement(statement_id)\n",
    "        print(f\"Statement completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing statement: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: orders\n",
      "Executing statement: 33a3c325-6dc6-4eb2-9da7-75364ffa11b0\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "âœ… Created table: orders\n",
      "Creating table: order_items\n",
      "Executing statement: d869ddee-d7dc-4d45-b2f3-fc9298c3eb8b\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement completed successfully\n",
      "âœ… Created table: order_items\n",
      "Creating table: payments\n",
      "Executing statement: 8d4a9501-2fe3-4ef4-8076-997ec24d65f8\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "âœ… Created table: payments\n",
      "Creating table: reviews\n",
      "Executing statement: 9ae3b5e1-4b8f-42fc-a128-f663dfc0f497\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement completed successfully\n",
      "âœ… Created table: reviews\n"
     ]
    }
   ],
   "source": [
    "# Create tables in Redshift\n",
    "def create_tables():\n",
    "    \"\"\"Create all necessary tables in Redshift\"\"\"\n",
    "    \n",
    "    # Orders table\n",
    "    orders_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id VARCHAR(255) PRIMARY KEY,\n",
    "        customer_id VARCHAR(255),\n",
    "        order_total DECIMAL(10,2),\n",
    "        order_status VARCHAR(50),\n",
    "        payment_method VARCHAR(50),\n",
    "        shipping_address TEXT,\n",
    "        created_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Order Items table\n",
    "    order_items_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS order_items (\n",
    "        order_item_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        product_id VARCHAR(255),\n",
    "        quantity INTEGER,\n",
    "        price DECIMAL(10,2)\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Payments table\n",
    "    payments_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS payments (\n",
    "        payment_id VARCHAR(255) PRIMARY KEY,\n",
    "        order_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        amount DECIMAL(10,2),\n",
    "        payment_method VARCHAR(50),\n",
    "        payment_status VARCHAR(50),\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reviews table\n",
    "    reviews_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reviews (\n",
    "        review_id VARCHAR(255) PRIMARY KEY,\n",
    "        product_id VARCHAR(255),\n",
    "        customer_id VARCHAR(255),\n",
    "        rating INTEGER,\n",
    "        created_at DATE\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    tables = {\n",
    "        'orders': orders_sql,\n",
    "        'order_items': order_items_sql,\n",
    "        'payments': payments_sql,\n",
    "        'reviews': reviews_sql\n",
    "    }\n",
    "    \n",
    "    for table_name, sql in tables.items():\n",
    "        print(f\"Creating table: {table_name}\")\n",
    "        run_redshift_statement(sql)\n",
    "        print(f\"Created table: {table_name}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "# Create tables\n",
    "create_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b248290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into orders from orders.csv\n",
      "Executing statement: c64160a7-56b5-4526-9aa8-0bb34e65ca17\n",
      "Statement status: PICKED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into orders\n",
      "Loading data into order_items from order_items.csv\n",
      "Executing statement: 2e5d8c6d-88ef-402c-b30b-b28493f72556\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into order_items\n",
      "Loading data into payments from payments.csv\n",
      "Executing statement: ba521b8b-1604-42bc-bf22-314350c99fee\n",
      "Statement status: PICKED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into payments\n",
      "Loading data into reviews from reviews.csv\n",
      "Executing statement: 5d827538-1bc5-4ffc-a9e8-bedbd4bf5cf6\n",
      "Statement status: PICKED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement status: STARTED, waiting...\n",
      "Statement completed successfully\n",
      "Loaded data into reviews\n"
     ]
    }
   ],
   "source": [
    "# Load data from S3 into Redshift tables\n",
    "def load_data_from_s3():\n",
    "    \"\"\"Load data from S3 CSV files into Redshift tables\"\"\"\n",
    "    \n",
    "    tables_and_files = {\n",
    "        'orders': 'orders.csv',\n",
    "        'order_items': 'order_items.csv',\n",
    "        'payments': 'payments.csv',\n",
    "        'reviews': 'reviews.csv'\n",
    "    }\n",
    "    \n",
    "    for table_name, file_name in tables_and_files.items():\n",
    "        print(f\"Loading data into {table_name} from {file_name}\")\n",
    "        \n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM 's3://{S3_BUCKET}/{file_name}'\n",
    "        IAM_ROLE '{redshift_role_arn}'\n",
    "        CSV\n",
    "        IGNOREHEADER 1\n",
    "        DELIMITER ','\n",
    "        REGION '{region}';\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            run_redshift_statement(copy_sql)\n",
    "            print(f\"Loaded data into {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data into {table_name}: {str(e)}\")\n",
    "\n",
    "# Load data from S3\n",
    "load_data_from_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e51221",
   "metadata": {},
   "source": [
    "## Step 5: Verify Data Load\n",
    "\n",
    "Let's verify that our data has been loaded correctly by running some sample queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea721a6",
   "metadata": {},
   "source": [
    "## Step 6: Create Bedrock Knowledge Base with Redshift Data Source\n",
    "\n",
    "Now we'll create the Bedrock Knowledge Base configured to use our Redshift data as a structured data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7891a188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base Name: redshift-structured-kb-0144320\n"
     ]
    }
   ],
   "source": [
    "# Configure Knowledge Base parameters\n",
    "kb_name = f\"redshift-structured-kb-{suffix}\"\n",
    "kb_description = \"Structured Knowledge Base for e-commerce data queries using Redshift\"\n",
    "generation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "print(f\"Knowledge Base Name: {kb_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7f9bc",
   "metadata": {},
   "source": [
    "Amazon Bedrock Knowledge Bases uses a service role to connect knowledge bases to structured data stores, retrieve data from these data stores, and generate SQL queries based on user queries and the structure of the data stores. There are several access patterns based on if you're using Redshift Serverless vs Redshift Provisioned Cluster. In this notebook, let's use `IAM Role + Redshift Serverless WorkGroup` access pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65137e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Knowledge Base parameters for Redshift Serverless with IAM authentication\n",
    "kb_config_param = {\n",
    "    \"type\": \"SQL\",\n",
    "    \"sqlKnowledgeBaseConfiguration\": {\n",
    "        \"type\": \"REDSHIFT\",\n",
    "        \"redshiftConfiguration\": {\n",
    "            \"storageConfigurations\": [{\n",
    "                \"type\": \"REDSHIFT\",\n",
    "                \"redshiftConfiguration\": {\n",
    "                    \"databaseName\": REDSHIFT_DATABASE\n",
    "                }\n",
    "            }],\n",
    "            \"queryEngineConfiguration\": {\n",
    "                \"type\": \"SERVERLESS\",\n",
    "                \"serverlessConfiguration\": {\n",
    "                    \"workgroupArn\": workgroup_arn,\n",
    "                    \"authConfiguration\": {\n",
    "                        \"type\": \"IAM\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "932cac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Step 1 - Creating Knowledge Base Execution Role (AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0144320) and Policies\n",
      "========================================================================================\n",
      "Step 2 - Creating Knowledge Base\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 20, 22, 20, 25, 102508, tzinfo=tzutc()),\n",
      "  'description': 'Structured Knowledge Base for e-commerce data queries using '\n",
      "                 'Redshift',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533267284022:knowledge-base/WZGWSWM6D6',\n",
      "  'knowledgeBaseConfiguration': { 'sqlKnowledgeBaseConfiguration': { 'redshiftConfiguration': { 'queryEngineConfiguration': { 'serverlessConfiguration': { 'authConfiguration': { 'type': 'IAM'},\n",
      "                                                                                                                                                           'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:533267284022:workgroup/7c19387f-5440-4e45-b76a-97e19e6920f5'},\n",
      "                                                                                                                              'type': 'SERVERLESS'},\n",
      "                                                                                                'storageConfigurations': [ { 'redshiftConfiguration': { 'databaseName': 'sds-ecommerce'},\n",
      "                                                                                                                             'type': 'REDSHIFT'}]},\n",
      "                                                                     'type': 'REDSHIFT'},\n",
      "                                  'type': 'SQL'},\n",
      "  'knowledgeBaseId': 'WZGWSWM6D6',\n",
      "  'name': 'redshift-structured-kb-0144320',\n",
      "  'roleArn': 'arn:aws:iam::533267284022:role/AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0144320',\n",
      "  'status': 'CREATING',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 20, 22, 20, 25, 102508, tzinfo=tzutc())}\n",
      "Creating Data Sources aka query engine\n",
      "{ 'createdAt': datetime.datetime(2025, 6, 20, 22, 20, 25, 253004, tzinfo=tzutc()),\n",
      "  'dataSourceConfiguration': {'type': 'REDSHIFT_METADATA'},\n",
      "  'dataSourceId': '9ENAZQQXRH',\n",
      "  'description': 'Query engine',\n",
      "  'knowledgeBaseId': 'WZGWSWM6D6',\n",
      "  'name': 'redshift-structured-kb-0144320-ds',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 20, 22, 20, 25, 253004, tzinfo=tzutc())}\n",
      "========================================================================================\n",
      "Knowledge Base created successfully!\n",
      "'WZGWSWM6D6'\n",
      "Knowledge Base ID: WZGWSWM6D6\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    structured_kb = BedrockStructuredKnowledgeBase(\n",
    "        kb_name=kb_name,\n",
    "        kb_description=kb_description,\n",
    "        workgroup_arn=workgroup_arn,\n",
    "        kbConfigParam=kb_config_param,\n",
    "        generation_model=generation_model,\n",
    "        suffix=suffix\n",
    "    )\n",
    "    \n",
    "    print(\"Knowledge Base created successfully!\")\n",
    "    kb_id = structured_kb.get_knowledge_base_id()\n",
    "    print(f\"Knowledge Base ID: {kb_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating Knowledge Base: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22c74e",
   "metadata": {},
   "source": [
    "## Step 6: Database Access Configuration for IAM Role + Redshift Serverless WorkGroup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00549fb8",
   "metadata": {},
   "source": [
    "For the IAM Role + Redshift Serverless WorkGroup access pattern, you must configure database-level permissions for the IAM role used by Bedrock Knowledge Base.\n",
    "\n",
    "1. **Create IAM-based database user**: Map the IAM role to a database user in Redshift\n",
    "2. **Grant appropriate permissions**: Provide SELECT access to the relevant schemas and tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "260dbdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Extracted Role Name: AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0144320\n"
     ]
    }
   ],
   "source": [
    "# Extract the IAM role name from the ARN for database user creation\n",
    "kb_details = structured_kb.knowledge_base\n",
    "\n",
    "bedrock_role_arn = kb_details['roleArn']\n",
    "bedrock_role_name = bedrock_role_arn.split('/')[-1]\n",
    "print(f\"   Extracted Role Name: {bedrock_role_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a7e47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0144320\n",
      "Executing statement: 06673198-2d70-4cc8-a8a9-719bbf657f10\n",
      "Statement status: SUBMITTED, waiting...\n",
      "Statement completed successfully\n",
      "IAM user created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the IAM user in Redshift (this is the critical missing step!)\n",
    "create_user_sql = f'CREATE USER \"IAMR:{bedrock_role_name}\" WITH PASSWORD DISABLE;'\n",
    "\n",
    "try:\n",
    "    print(f\"Creating user: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(create_user_sql)\n",
    "    print(\"IAM user created successfully!\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"User already exists, continuing...\")\n",
    "    else:\n",
    "        print(f\"Error creating user: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ab5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granting SELECT permissions to: IAMR:AmazonBedrockExecutionRoleForStructuredKnowledgeBase_0144320\n",
      "Executing statement: d3b2c89b-b1fb-45e0-8985-12cda4446d27\n",
      "Statement status: PICKED, waiting...\n",
      "Statement completed successfully\n",
      "SELECT permissions granted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Grant SELECT on all tables in public schema\n",
    "grant_select_sql = f'GRANT SELECT ON ALL TABLES IN SCHEMA public TO \"IAMR:{bedrock_role_name}\";'\n",
    "\n",
    "try:\n",
    "    print(f\"Granting SELECT permissions to: IAMR:{bedrock_role_name}\")\n",
    "    run_redshift_statement(grant_select_sql)\n",
    "    print(\"SELECT permissions granted successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error granting permissions: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552d4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for querying the Knowledge Base\n",
    "\n",
    "def query_with_retrieve_and_generate(kb_id, query):\n",
    "    \"\"\"Query using retrieve_and_generate API - returns natural language response\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "            input={\"text\": query},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                \"type\": \"KNOWLEDGE_BASE\",\n",
    "                \"knowledgeBaseConfiguration\": {\n",
    "                    'knowledgeBaseId': kb_id,\n",
    "                    \"modelArn\": f\"arn:aws:bedrock:{region}::foundation-model/{generation_model}\",\n",
    "                    \"retrievalConfiguration\": {\n",
    "                        \"vectorSearchConfiguration\": {\n",
    "                            \"numberOfResults\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response['output']['text']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_sql_query(kb_arn, query):\n",
    "    \"\"\"Generate SQL query from natural language\"\"\"\n",
    "    try:\n",
    "        response = bedrock_agent_runtime_client.generate_query(\n",
    "            queryGenerationInput={\n",
    "                \"text\": query,\n",
    "                \"type\": \"TEXT\"\n",
    "            },\n",
    "            transformationConfiguration={\n",
    "                \"mode\": \"TEXT_TO_SQL\",\n",
    "                \"textToSqlConfiguration\": {\n",
    "                    \"type\": \"KNOWLEDGE_BASE\",\n",
    "                    \"knowledgeBaseConfiguration\": {\n",
    "                        \"knowledgeBaseArn\": kb_details['knowledgeBaseArn']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.get('queries') and len(response['queries']) > 0:\n",
    "            return response['queries'][0]['sql']\n",
    "        else:\n",
    "            return \"No SQL generated\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6ad67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Knowledge Base with sample queries\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Query 1: How many orders are in the database?\n",
      "--------------------------------------------------\n",
      "ğŸ¤– Natural Language Response:\n",
      "   There are 10000 orders in the database.\n",
      "\n",
      "ğŸ”§ Generated SQL:\n",
      "   SELECT COUNT(*) AS \"total_orders\" FROM public.orders;\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ“ Query 2: What is the average order total?\n",
      "--------------------------------------------------\n",
      "ğŸ¤– Natural Language Response:\n",
      "   The average order total is $507.84.\n",
      "\n",
      "ğŸ”§ Generated SQL:\n",
      "   SELECT AVG(\"order_total\") AS \"average_order_total\" FROM public.orders;\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How many orders are in the database?\",\n",
    "    \"What is the average order total?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing Knowledge Base with sample queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nğŸ“ Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get natural language response\n",
    "    print(\"ğŸ¤– Natural Language Response:\")\n",
    "    nl_response = query_with_retrieve_and_generate(kb_id, query)\n",
    "    print(f\"   {nl_response}\")\n",
    "    \n",
    "    # Get generated SQL\n",
    "    print(\"\\nğŸ”§ Generated SQL:\")\n",
    "    sql_query = generate_sql_query(kb_details['knowledgeBaseArn'], query)\n",
    "    print(f\"   {sql_query}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bd6df",
   "metadata": {},
   "source": [
    "## Step 7: Start Ingestion Job\n",
    "\n",
    "Now that the database permissions are properly configured, let's start the ingestion job to sync the data from the Redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097a5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job  started successfully\n",
      "\n",
      "{ 'dataSourceId': '9ENAZQQXRH',\n",
      "  'ingestionJobId': '7O6GZYMXXE',\n",
      "  'knowledgeBaseId': 'WZGWSWM6D6',\n",
      "  'startedAt': datetime.datetime(2025, 6, 20, 22, 28, 0, 624068, tzinfo=tzutc()),\n",
      "  'status': 'FAILED',\n",
      "  'updatedAt': datetime.datetime(2025, 6, 20, 22, 28, 4, 95306, tzinfo=tzutc())}\n",
      ".....\n",
      "Ingestion job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Wait a bit for the Knowledge Base to be fully ready\n",
    "time.sleep(20)\n",
    "structured_kb.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ba69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
